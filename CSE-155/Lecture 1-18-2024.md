# A Brief History of HCI
- Take inspiration from natural actions
	- Interactions with physical objects -> implement into computer systems
	- Exploit things in nature and imitate their actions
- Know how to look -> fast lookup times
	- ### Vannevar Bush: Hypertext
		- Yes, that HT in HTTP, before the internet
		- Use links and associative trails to connect documents together
		- The memex machine
			- Could "think," before the advent of an actual search engine
			- A camera that could capture images, send it to the memex (microfilm library + lookup system)
			- Add comments and new data into the system as well, by writing to the microfilm
		- Concept of non-linear association: cycles and graphs as documents associate with other documents.
- Think by association when interacting with an information-rich environment
	- Stimuli, either from bursts of light, readable documents, or both (e.g, a screen)
	- Why not a touch-screen instead of physical buttons to manually search?
		- ### Sketchpad: Ivan Sutherland
			- Device was large, used a dedicated light pen, and had a small drawing space
			- Even bigger of an issue, to end a drawing, you had to pull the pen away: the active state and non-state would be blurred depending on how far the pen would be
			- Also hurts to use, your arm would get tired drawing on a vertical surface (or carpal tunnel)
			- But, it was one of the first ways to visualize and interact with objects directly! You can directly input information, rather than sending commands to manipulate things indirectly.
				- Incremental building (draw one line at a time)
				- Visible results as well - not just text (e.g., LaTeX vs actual compiled document)
					- Note that a radio with knobs and a position indicator is considered visible, even if it has no display - a headless server would be considered non-visible since it has no monitor.
				- You know when your actions are starting (e.g., starting to draw a line - rapid feedback)
				- Direct manipulation also allows undoing - reverse the stack: reversibility
				- Exploration: allow pure actions to run, e.g., trying things without any consequence
					- Draw a line, then undo, or moving your cursor in viewing mode, or just viewing the results without mutating the file
				- Syntactic correctness: it should *work* and be correct
				- Replacing language with action: don't write code to do something, just *do* it
					- Graphical interfaces vs scripting/terminal interfaces
			- Attributes of direct manipulation^^^ (versus indirect, e.g., programming)
				- The concept of WYSIWYG (what you see is what you get)
	- OK, my hand is tired, what's better than a light pen?
		- ### A mouse! - Douglas Engelbart (1963)
			- Called *The Mother of All Demos*
			- Uses two potentiometers (perpendicular) to capture positioning data
				- Compared to ball mice: also two potentiometers
				- Compared optical mice: lasers (no moving parts!)
			- Former two use ADC (analog-to-digital converters), latter use image processing to detect movement (more complex!)
	- What about real touchscreens?
		- ### Touchscreen - Eric Johnson (1965)
		- Uses conductive wire to detect touching against a CRT
			- No multi-touch since it was capacitive (or resistive? basically only one finger)
		- Useful as a keyboard, and as a button
			- But, not much physical feedback (no tactile feeling), and precision even worse
				- If you're wearing gloves, sucks to be you (capacitive screen, barely works on it, and you can't feel through the glove)
			- You must look at the screen to use a touch screen - you can feel your way around a button interface, but touch screens don't let you feel without accidentally interacting (thanks Tesla!)
				- Apple Keyboard simulator
			- But, easy to implement!
	- What about AR/VR?
		- ### Sword or Damocles - Ivan Sutherland (1968)
			- Named that way because it's so heavy that it could snap someone's neck (hanging over their head, since black)
		- Augmented reality
			- Sensors to detect person's perspective
			- Draws an image (a box) upon the scene viewed, like an overlay
		- Primitive in design and interaction, but validates the concept of AR
- Comparing input methods:

| Input type | Functionality | Usability | Commonality |
| ---- | ---- | ---- | ---- |
| Mouse | 2D positioning, by moving (on table) | Easy | you know this already |
| Joystick | 2D positioning, fixed | Easy | Anything that requires analog input |
| Lightpen | 2D positioning, by moving (vertical) | Easy, but tiring | tf are you a boomer |
| Grafacon | 2D positioning, like the lightpen but on a tablet form - basically the drawing pad | what the fuck | literally only for Osu |
| Knee Control | 1D? positioning | double tf | i cut off your arms |
| Pedals | Press buttons - may have analogue input | Easy | Mostly for instruments nowadays, to emulate their behavior |
| Touchscreen | 2D positioning and possible multi-touch | Easy-ish? | Phones! |
OK, what did we get out of figuring out which input methods are the best?
- ### The Xerox Star (1981)
	- Keyboard and mouse control, with a graphical interface
	- Only for the low-low price of $15,000 (in 1981 money)
	- Where's the apps? It's a cool platform but wtf can I do with it
		- Can't even run DOOM smh my head, no business software too i guess
	- But, it had a graphical UI that looks pretty close to modern Windows, and even had a desktop and file system
		- The icons were also obvious - know what's a file, folder, drive, application, etc.
			- Even if internally they were all files!
		- These are used in the desktop environment, hence the *desktop metaphor* to make it relatable
			- Not useful in 2024 since we don't use floppy disks anymore...
	- High research to make it usable, but the closed architecture made it hard to use and develop for
	-  Event-driven vs non-event-driven architecture
		- Xerox uses events? i.e., all actions from the human result (i.e., an event) will run a task, which can spawn more events to trigger other tasks, and so on
	- Defeated by the Apple Lisa, since it actually had applications
		- Also expensive but it was usable
	- Icons were also obvious, although for the time period
		- Floppy disks, file cabinets, terminals...

ACM: I forgor

HCI: an introduction: Card, Moran, Newell
- Long-term memory, working memory, visual/audio/computation latency
- Try to model a human's actions, interactions, and even predict how they would react given specific stimuli

The Apple Macintosh (1984)
- Good UI!
	- Dropdown that are always visible and named
	- Change what's available depending on context (e.g., selected content or application)
		- Options can say what they do, but also *show* what they do (e.g., **bolded** text, *italic* text in the option names themselves)
		- Considered a dual-function interface, with both control and display functions (show what it does, and DO what it does)
- Apple and other companies created software, making it useful
- The operating system was clean, easy to use, if aside from hitting a kernel bug
	- Lots of money put into it!

A better touchscreen: multi-touch: Mehta, Smith (1982)
- Well, supposed to be, but using vision processing to determine position? It was also expensive and bulky.
	- Used shadows from touching glass against a backlight
- No idea why multitouch was needed though, just a research experiment
- Lee, Buxton, Smith (1985): What if we made multitouch useful?
	- Set a lot of things depending on how many fingers to use
	- Also simulate force detection by seeing how much area is being covered
		- But for now, why not a load sensor?
		- Can also do a 3D sensing by using pressure
	- Needs to scan each point discretely, which is slow
	- Demo showed examples of pressure-sensitive brushes and multi-touch, as well as using a single touch surface to represent multiple separate inputs (e.g., the left side of the tablet can control one switch, right side controls another)

OK, but where can we *use* touchscreens?
- PDAs first, then smart phones
- Easier to use grid interfaces by clicking, faster to use
- Also games I guess - it was easy to touch and swipe than to press a ton of buttons (rip BlackBerry physical keyboards)

What if we made physical touchscreens?
- Use tangible objects to interact with the computer: graspable interfaces
	- Tangibles: Fitzmaurice, Ishii, Buxton (1995)
- Tangible, Graspable, all the same thing - physical items that also transverse across into the digital world
	- Problem found: with virtual interfaces, you are forced to do things serially: transformation requires translating the object first (moving it), then rotating it
		- With a physical object, we can do both simultaneously in the same action
	- How do we parallelize everything? A tangible object can help by mapping physical objects to virtual environments
		- But we need to give them more degrees of freedom
- Even more: How do we adjust interfaces depending on the interface?
	- App update - new interface: physical interfaces are fixed, but virtual interfaces can be adapted per application
	- Touch screens might not give tangible feedback, but they update as fast as the screen
		- Natural as well, since you can touch them