### Dyslexia:
- 1/10 people have dyslexia
	- They'll confuse the order of words, put letters the wrong way around (physically, e.g., "b" instead of "d")
	- Better at verbal comprehension than text
	- Find it hard to carry out a sequence of directions (possibly out of order)
		- Anytime with a linear relationship with subactions, they can skip as well.
	- They struggle with planning and organization
	- Often have good skills in other areas, such as creative thinking and problem solving
		- Can be exploited for UI decisions
- So how to design interface for it?
	- Support readability tools
		- Avoid text in images (have it plain-text)
		- Avoid CSS hell (make it semantic, not visual)
			- Elements that are supposed to be prominent should be in HTML or aria tags, not CSS font size and using the same tag for everything
			- Could be uninterpretable by the screen reader/extensions
		- Support flexible navigation with a variety of paths
			- Not just one way to reach somewhere, but multiple ways
				- Intuition is different between people
			- Easy for non-dyslexic people as well, since you don't need to read a manual or memorize
	- Use Dyslexic-friendly fonts
		- Reduce in-word character separation
		- Increasing between word separation
		- Avoid italics and underlining, since they may be percieved as extra typesetting "information"
			- Feels like a CAPTCHA
		- Tweaking similar looking letters to be more distinct (wobbles or elongations)
		- Extend vertical components (e.g., "f", "h", "p") of certain letters to make them more prominent and harder to mix among
		- If you don't have access to a dyslexic friendly font, there are alternatives
			- Eccentric font choice
			- Avoid similar letter design
### Eye tracking (wtf why are we here)

- The eye is a sensor, but what if we used it as a responder?
- Very old: started in 1879 by Louis Emile Javal (ew, French people in Paris)
	- an ophthalmologist
- We don't follow data in a linear path, our eyes jump to specific points that we assume significant information exists
	- e.g., colored words, bold fonts, large size, distinct contrast
	- You don't read a sentence from left to right, you actually look back and forward, mostly to skim informationbut also gain inference before going in detail
		- also why i can't focus
	- Observed that readers eyes don't skim in a straight line
	- Rapidly move our eyes (saccades) and focus on something for more information (fixations)
		- Saccade: rapid repositioning of the eye to fixate on a new location
			- quick: 120ms
			- We need this to understand general information - we don't choose to do that, we can force it, but this is mostly involuntary by the eye and we don't even control this saccade effect
			- Ofc, we can override it, hence the eyedoctor test to follow their finger or a pointer
		- Fixation
			- Eyes are stationary (dwelling over something)
			- Taking in visual detail from the environment
				- Noted as "staring at the thing"
			- Long or short: typically, at least 200 ms
- Yarbus' Study (1965)
	- Alfred Lukyanovich Yarbus (soviet psychologist)
	- Pioneered the study of saccadic exploration of complex images
		- Trajectories followed by the gaze depend on the task that the observer has to perform
			- E.g., summarization vs counting vowels: different tasks, we can differently to optimize how we collect data
		- The gaze tends to jump back and forth between the same parts of a scene
			- Human nature, we can't keep focusing at a point - we get bored and want to look at something else
	- If told to look at a painting freely, there is no specific pattern that everyone goes through, but there are key things that we stare at
		- Faces, objects, background, furniture
	- But if asked for predicting ages, we focus on faces, because that's where most of the information can get
		- e.g., face wrinkles
		- more information can be received from say, height - hence some points looking at their feet to approximate height
		- We don't care about the background because it's useless - faical data gives us the most, hence why our eyes track that more
	- How the hell did Yarbus capture this data?
		- Huge-ass contraption, where you cannot move your body so your eyes could be in the same place
			- shines light on your eyeballs
		- Suction cup on your eyeball? which has a mirror attached and the twitching of the eyes can be recorded by a photosensor (light)
			- That shit probably hurts
		- And this also falls in ethics lol, like 5 minutes on this
		- But, this was a primitive way of eye tracking without computer tracking technology - the mirror would drastically show where your eyes were pointing
	- Modern eye tracking
		- Two types of feature-based eye-tracking methods
			- Interpolation-based
				- Uses polynomial regression or projective geometry in a 2D plane
			- Model-based
				- Uses eye features to create a 3D geometric model of the eye in the 3D space
		- Commercial eye tracking simply creates a pattern of infrared illumination, so you don't have to see bright light
		- The cameras take images of your eyes and note how your eyes are moving in the scene
		- Image processing algorithms look for details and reflections from the eyes to determine how they're moving in 3D
		- Knowing the reflection, a construction of the eyes in 3D space can be formed, allowing the device to know where the eyes are looking at and record that information in real time to understand fixations and movement/tracking
		- Evaluating eye tracking
			- Calibration, offset, error (gaze estimation, +/- 0.5 degrees is considered high accuracy)
			- Use the corneal reflection to determine if the eye is gazing in a specific direction
				- Think of a cat's eye when you see them in the dark
				- Blast IR and see where the reflection is relative to the eyeball
			- Only 2D but you can have a second camera to determine 3D positinoing
		- General gist is that you need one camera for 2D, two cameras for 3D
			- Use IR for eye tracking so you don't blind the person
			- Calibration to adjust parameters for real life
			- Offset because the camera is somewhere else, not in front of the person directly
			- Find the reflection of the eyes (corneal reflection) to determine what they're looking at to their pupil
			- Use math to either 3D model the eye in space, or simply calculate the angle they're looking at
		- OK, this dude just said "deterMINE" like mining 
		- Use angles to determine error rate to see the difference between true and predicted data to tell if the eye tracking system is good or not
			- A range most likely, since either it can be pretty good, sometimes good, or shit
			- Fix it with better sensors, calibration, offsets, or algorithm
			- High resolution sensors can auto-calibrate and determine things like distance, meaning that distance is no longer an issue
		- Scan patterns: circles or spirals, idk i blanked out on this slide
		- Applications:
			- User behavior research (e.g., reading patterns)
			- Marketing research (e.g., ad placement)
			- Input and interaction research
			- Damn all this stuff makes me hate technology
			- Marking information can be used to collect data in certain patterns
				- Our eyes can focus on details (see earlier on color/size that influence our scanning behavior to force us to fixate)
				- Lots of research because people want to make money from getting user attention
			- How to make your website/UI better? put important things at highly viewed scan points
				- Quickly find what you're looking for rather than wasting time scanning
			- Make a keyboard that uses vision, in case you want to have dry eye syndrome
				- Probably useful for people who are mostly paralyzed.
				- Harder to use since there has to be a difference between gazing/scanning and selecting - the easiest way is dwelling, but it's also harder since it's not natural
					- Dwelling is hard (how to press same key, wait like multiple seconds)
					- Blinking is hard (on command is tiring)
			- How do we make this UI/UX better than staring at a keyboard?
				- the entire thing fell into a VR/AR dicussion about technology and brands
				- Multi-modal interaction: why not use both eye tracking and say, speech?
					- Or not even speech: Ahmed plugging his research for the IIL
						- Silent speech: the camera reads both your eyes and lip movement
							- Use eyes to do general tracking, use voice to enable gravity/specialized control or input
							- 96% more accurate than eyes/dwelling alone, 9% faster
			## Hearing (different slides, wtf)
			- Hearing (audition)
				- Sound is cyclic fluctuations of pressure in a medium, such as air
				- Created when physical objects are moved or vibrated
					- A way for us to perceive sound
			- Auditory Stimulus
				- Physical properties of sound
					- Amplitude
					- Frequency
					- Timbre or tone color (pure frequency or wide)
					- Duration
				- Create subjective properties of hearing
					- Loudness
					- Pitch
					- Richness
					- Tempo or rhythm
				- Very low and high frequencies are *perceived* to be softer than sounds in the middle frequency, even at the same amplitudes
				- Frequency
					- Allows us to tell if sounds are higher or lower
					- High-pitched sound causes molecules to rapid oscillate
					- Pitch can only be determined when a sound has a frequency that is clear and consistent enough to differentiate it from noise
						- what is this, a fourier transform
					- Perception:
						- Pitch is primarily based on a listener's perception
							- Subjective, not objective - hence why there are people who can't sing
						- Not an objective physical property of sound
				- Timbre (richness)
					- Sounds with various timbres produce different wave shapes
					- Affect our interpretation of the sound
						- Harmonic structure of sound
					- E.g., square wave, sine, different instruments
				- Duration (tempo/rhythm)
					- Duration is how long a pitch or tone lasts - short or long
					- Also affects the timbre and rhythm of a sound
						- e.g., classical has longer notes vs those of a keyboardist (i disagree but that's on the slides)
						- Assists in distinguishing notes of the same pitch coming from different instruments
					- The duration of a sound or tone begins once the sound registers and ends after it cannot be detected
				- Why sound? provide auditory feedback
					- Like said in the previous lecture, don't use color alone
					- Good discriminator and easy to remember/identify, but not the only way
					- Provide auditory feedback, when appropriate (e.g., flash/blink + beep upon an error)
						- Different auditory feedback can signal different things (e.g., error noises, warning, info)
							- We can use sounds that exist in real life to signal different features
							- Or sounds copied/mutated from existing systems (e.g., Windows error tones) to signal different things
							- Basically, abuse our sound association to signal different events, try to bring from nature, or use artificial sounds if they're unique events (but make them obvious)
							- (Associative learning)
								- Don't produce something novel if nobody can understand what it's supposed to be, usually a bad idea
									- Exploit something that people already know, to enhance usability and reduce human error
					- Note: some people are deaf
					- Windows boot sounds were long, and now they're short
						- Turns out computers are booting faster, so we have no time for bootup sounds lol
							- The earlier ones were for professional - we don't even have room on the disk for sounds
								- Let alone they didn't care about sounds, or attract their attentions: it was a specialized set of users
							- Middle Windows versions (consumer editions), a way to attract their attention to computing in general
							- Late Windows: too long, stressed, less sound needed - make it short!
								- Avoid excessive number of information to avoid information overflow
								- Possibly also shorter attention spans
						- Comparison: pretend it's a fast boot up by taking a screen shot and show it to deceive the user for being "ready"
							- thanks Apple, but tbf it *worked* at tricking people into thinking macs were faster
							- Also, chime on pressing the button is basically cheating at "fast bootup" compared to waiting for the Windows chime for the OS
						- Auditory feedback for LVB
							- Low vision and blind people
							- Usually combined with other feedback, e.g., haptics
								- Multi-modal - multiple ways to control, more degrees of freedom since it doesn't have to be one input device (e.g., sound + voice + feeling + tactile feedback)
							- Increase speed by reducing duration
								- Faster output and faster input - assuming the human can catch up
									- Should be controllable by events
							- Increase learnability through intuitiveness and association